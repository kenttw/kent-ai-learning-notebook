{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert LightGBM Model to Java Code and SQL Code\n",
    "In some real-life cases, we want to deploy the ML model quickly but this production system environment is very complicated. So we could use this tool that converts this LightGBM model to native java code or native SQL code. Those two languages are very popular in the legacy system. Deploy those native model codes without any ML framework. This tool could help everybody to turn all your Machine Learning model to productionization quickly. Wish this tool could make you happy and cozy ^^ !."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm2Java, lightgbm2SQL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier with titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 268, number of negative: 444\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 197\n",
      "[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.376404 -> initscore=-0.504838\n",
      "[LightGBM] [Info] Start training from score -0.504838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kentshih/PycharmProjects/kent-ai-learning-notebook/venv/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/kentshih/PycharmProjects/kent-ai-learning-notebook/venv/lib/python3.8/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/kentshih/PycharmProjects/kent-ai-learning-notebook/venv/lib/python3.8/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "/Users/kentshih/PycharmProjects/kent-ai-learning-notebook/venv/lib/python3.8/site-packages/lightgbm/callback.py:183: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df = pd.read_csv('https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv')\n",
    "test_df = pd.read_csv('https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/test.csv')\n",
    "train_df.head()\n",
    "y = train_df.pop('Survived')\n",
    "cols = ['Pclass', 'Age', 'Fare', 'SibSp', 'Parch','Sex','Embarked']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df[cols],\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "for c in ['Sex', 'Embarked']:\n",
    "    X_train[c] = X_train[c].astype('category')\n",
    "    X_test[c] = X_test[c].astype('category')\n",
    "\n",
    "\n",
    "# Create an LGBM dataset for training\n",
    "train_data = lgb.Dataset(data=X_train[cols],\n",
    "                        label=y_train)\n",
    "\n",
    "# Create an LGBM dataset from the test\n",
    "test_data = lgb.Dataset(data=X_test[cols],\n",
    "                        label=y_test)\n",
    "lgb_params = {\n",
    "    'boosting': 'dart',          # dart (drop out trees) often performs better\n",
    "    'application': 'binary',     # Binary classification\n",
    "    'learning_rate': 0.05,       # Learning rate, controls size of a gradient descent step\n",
    "    'min_data_in_leaf': 20,      # Data set is quite small so reduce this a bit\n",
    "    'feature_fraction': 0.7,     # Proportion of features in each boost, controls overfitting\n",
    "    'metric': 'binary_logloss',  # Area under ROC curve as the evaulation metric\n",
    "    'drop_rate': 0.15,\n",
    "    'n_estimators' : 5,\n",
    "    'num_leaves': 4,\n",
    "    'max_depth': 3,\n",
    "}\n",
    "\n",
    "evaluation_results = {}\n",
    "model = lgb.train(train_set=train_data,\n",
    "                params=lgb_params,\n",
    "                valid_sets=[train_data, test_data],\n",
    "                valid_names=['Train', 'Test'],\n",
    "                evals_result=evaluation_results,\n",
    "                num_boost_round=500,\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval=20,\n",
    "\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert2Java "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import java.lang.Math;        \n",
      "\n",
      "double predictTree0(double Pclass, double Age, double Fare, double SibSp, double Parch, String Sex, String Embarked) { if ( Sex.equals(\"male\") ||  2==1 ) { if ( Age <= 6.500000000000001 ) { return -0.43977958328510247; } else { return -0.5505522531377166; } } else { if ( Fare <= 47.00000000000001 ) { return -0.44341053776981176; } else { return -0.38743770800794064; } } }\n",
      "\n",
      "double predictTree1(double Pclass, double Age, double Fare, double SibSp, double Parch, String Sex, String Embarked) { if ( Sex.equals(\"male\") ||  2==1 ) { if ( Age <= 6.500000000000001 ) { return 0.060855060145106304; } else { return -0.043957340673887485; } } else { if ( SibSp <= 1.5000000000000002 ) { return 0.07994095823701186; } else { return 0.015206689695068991; } } }\n",
      "\n",
      "double predictTree2(double Pclass, double Age, double Fare, double SibSp, double Parch, String Sex, String Embarked) { if ( Sex.equals(\"male\") ||  2==1 ) { if ( Age <= 6.500000000000001 ) { return 0.05708752427825118; } else { if ( Fare <= 26.268750000000004 ) { return -0.05305258348576311; } else { return -0.011755823383138858; } } } else { return 0.06745206741796382; } }\n",
      "\n",
      "double predictTree3(double Pclass, double Age, double Fare, double SibSp, double Parch, String Sex, String Embarked) { if ( Sex.equals(\"male\") ||  2==1 ) { if ( Age <= 6.500000000000001 ) { return 0.05368450910545441; } else { if ( Fare <= 26.268750000000004 ) { return -0.05123206990461915; } else { return -0.01120737766913426; } } } else { return 0.06337543032594332; } }\n",
      "\n",
      "double predictTree4(double Pclass, double Age, double Fare, double SibSp, double Parch, String Sex, String Embarked) { if ( Pclass <= 2.5000000000000004 ) { if ( Fare <= 52.27710000000001 ) { if ( Parch <= 1.0000000180025095e-35 ) { return 0.0035427525505176724; } else { return 0.07007106771786567; } } else { return 0.06571339896149972; } } else { return -0.02741428427540888; } }\n",
      "\n",
      "double predict(double Pclass, double Age, double Fare, double SibSp, double Parch, String Sex, String Embarked) { double score= predictTree0( Pclass,  Age,  Fare,  SibSp,  Parch,  Sex,  Embarked) + predictTree1( Pclass,  Age,  Fare,  SibSp,  Parch,  Sex,  Embarked) + predictTree2( Pclass,  Age,  Fare,  SibSp,  Parch,  Sex,  Embarked) + predictTree3( Pclass,  Age,  Fare,  SibSp,  Parch,  Sex,  Embarked) + predictTree4( Pclass,  Age,  Fare,  SibSp,  Parch,  Sex,  Embarked);\n",
      "return  1/(1+ Math.exp(score*-1));\n",
      "                 \n",
      "                \n",
      "                \n"
     ]
    }
   ],
   "source": [
    "lg2java = lightgbm2Java.Lightgbm2Java()\n",
    "code = lg2java.doProcess(model.dump_model())\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert2SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create view score_view as \n",
      "(\n",
      "select id,`Pclass`,`Age`,`Fare`,`SibSp`,`Parch`,`Sex`,`Embarked`,\n",
      "(case when ( `Sex`='male' or  2=1 )  \n",
      "then  case when ( Age <= 6.500000000000001 )  \n",
      "then   -0.43977958328510247  \n",
      "else  -0.5505522531377166  end  \n",
      "else case when ( Fare <= 47.00000000000001 )  \n",
      "then   -0.44341053776981176  \n",
      "else  -0.38743770800794064  end  end) +\n",
      "\n",
      "(case when ( `Sex`='male' or  2=1 )  \n",
      "then  case when ( Age <= 6.500000000000001 )  \n",
      "then   0.060855060145106304  \n",
      "else  -0.043957340673887485  end  \n",
      "else case when ( SibSp <= 1.5000000000000002 )  \n",
      "then   0.07994095823701186  \n",
      "else  0.015206689695068991  end  end) +\n",
      "\n",
      "(case when ( `Sex`='male' or  2=1 )  \n",
      "then  case when ( Age <= 6.500000000000001 )  \n",
      "then   0.05708752427825118  \n",
      "else case when ( Fare <= 26.268750000000004 )  \n",
      "then   -0.05305258348576311  \n",
      "else  -0.011755823383138858  end  end  \n",
      "else  0.06745206741796382  end) +\n",
      "\n",
      "(case when ( `Sex`='male' or  2=1 )  \n",
      "then  case when ( Age <= 6.500000000000001 )  \n",
      "then   0.05368450910545441  \n",
      "else case when ( Fare <= 26.268750000000004 )  \n",
      "then   -0.05123206990461915  \n",
      "else  -0.01120737766913426  end  end  \n",
      "else  0.06337543032594332  end) +\n",
      "\n",
      "(case when ( Pclass <= 2.5000000000000004 )  \n",
      "then  case when ( Fare <= 52.27710000000001 )  \n",
      "then  case when ( Parch <= 1.0000000180025095e-35 )  \n",
      "then   0.0035427525505176724  \n",
      "else  0.07007106771786567  end  \n",
      "else  0.06571339896149972  end  \n",
      "else  -0.02741428427540888  end) \n",
      "\n",
      "as score \n",
      "from raw_data)\n",
      ";\n",
      "\n",
      "create view result_view as \n",
      "select `Pclass`,`Age`,`Fare`,`SibSp`,`Parch`,`Sex`,`Embarked`,\n",
      "cast( 1/(1+EXP(-1*score)) as double) from score_view\n",
      ";\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "lg2sql = lightgbm2SQL.Lightgbm2SQL()\n",
    "code = lg2sql.doProcess(model.dump_model())\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}